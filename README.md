
**gpt2-moe-detector**

---

# GPTâ€‘2 Text Detection via Heterogeneous Mixtureâ€‘ofâ€‘Experts

A Python implementation of a fourâ€‘expert Mixtureâ€‘ofâ€‘Experts (MoE) model to distinguish between humanâ€‘written text and GPTâ€‘2â€‘generated text. Combines lexical, semantic, stylometric, and zeroâ€‘shot perplexity cues with a learned gating network for robust, interpretable detection.

---

## ğŸš€ Features

- **Lexical Expert**  
  TFâ€‘IDF vectorization + logistic regression for nâ€‘gram anomaly detection.

- **Semantic Expert**  
  Fineâ€‘tuned DistilBERT classifier for deep contextual understanding.

- **Stylometric Expert**  
  Random forest on structural/style features (sentence length, TTR, entropy, etc.).

- **Zeroâ€‘Shot Expert**  
  Maps GPTâ€‘2 perplexity via a logistic functionâ€”no additional training required.

- **Gating Network**  
  Softâ€‘EM trained MLP with learnable temperature scaling to route each sample to the most confident expert.

- **Interpretable Routing**  
  Perâ€‘sample expert assignments and confidence scores enable dynamic humanâ€‘inâ€‘theâ€‘loop review.

## â–¶ï¸ Quickstart

Launch the Colab notebook for an endâ€‘toâ€‘end walkthrough:

```bash
jupyter notebook notebooks/gpt_moe.ipynb
```

Key steps include:

1. **Data Loading & Sampling**
2. **Feature Extraction**
3. **Expert Training (LogReg, RF, DistilBERT)**
4. **Zeroâ€‘Shot Perplexity Scoring**
5. **Gating Network & EM Training**
6. **Final Evaluation**

Results (on heldâ€‘out test set):

```
Fullâ€‘scale (160â€¯K/20â€¯K/20â€¯K): Acc = 0.9058, F1 = 0.9024  
Miniâ€‘run (2â€¯K samples):      Acc = 0.8935, F1 = 0.8898  
```

---

## ğŸ“Š Results & Analysis

* **Expert Utilization**

  * DistilBERT & Stylometric RF dominate, TFâ€‘IDF and zeroâ€‘shot lightly used.
* **Errorâ€‘Bucket Analysis**

  * Accuracy stratified by gating confidence bins (see appendix).
* **Interpretable Insights**

  * Perâ€‘sample gating weights reveal when to defer to human review.

---

## ğŸ” Architecture Overview

1. **Experts**

   * Each expert outputs `[P(human), P(AI)]`.
2. **Metaâ€‘Features**

   * Entropy & margin for each expert â†’ 8 dims.
   * Concatenate with DistilBERT embedding and stylometric vector.
3. **Gating MLP**

   * 2â€‘layer network + BatchNorm, Dropout, temperatureâ€‘scaled softmax.
4. **EM Loop**

   * Eâ€‘step: compute responsibilities.
   * Mâ€‘step: reâ€‘train LogReg & RF with weights, fineâ€‘tune DistilBERT, update gate.


## âœ‰ï¸ Contact

Cooper Gilkey â€” [cooperjgilkey@gmail.com](mailto:cooperjgilkey@gmail.com)
GitHub: [github.com/CooperJRG](https://github.com/CooperJRG)
LinkedIn: [linkedin.com/in/cooper-gilkey](https://www.linkedin.com/in/cooper-gilkey)
